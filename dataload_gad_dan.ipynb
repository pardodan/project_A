{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataload_gad_dan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef73fd26ee914ecdb22046ed28d35daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b06620a8a554af190b006df76f66f2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_81c902b0ff0b4c4c83128f6f517407ff",
              "IPY_MODEL_41bda20f9c3a49649ad01939b49550c7"
            ]
          }
        },
        "4b06620a8a554af190b006df76f66f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81c902b0ff0b4c4c83128f6f517407ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f5f2d35c7af74b689d4e589505de4bf2",
            "_dom_classes": [],
            "description": "  9%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fad7044a1bf4ce69301929193f6230f"
          }
        },
        "41bda20f9c3a49649ad01939b49550c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a309c81474a473ba594d21a12a9f1d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15343616/170498071 [00:00&lt;01:20, 1937282.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8fe8866e06243b3b18b23ba390bef5b"
          }
        },
        "f5f2d35c7af74b689d4e589505de4bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fad7044a1bf4ce69301929193f6230f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a309c81474a473ba594d21a12a9f1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8fe8866e06243b3b18b23ba390bef5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pardodan/project_A/blob/main/dataload_gad_dan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTTwtM1m9qAx"
      },
      "source": [
        "# **Dataset loader**\r\n",
        "this are the attemps of the data loader to load and preper the data before running it in the train model\r\n",
        "\r\n",
        "there are 2 main challenges in this script:\r\n",
        "\r\n",
        "1) load the data with the proper labels from the server \r\n",
        "\r\n",
        "2) crop the pictures to a \"specific\" size before running it it the model\r\n",
        "\r\n",
        "#######TRY_1########"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWEhRE0x7px-"
      },
      "source": [
        "##----------- imports ------------##\r\n",
        "\r\n",
        "# torch\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "import torchvision\r\n",
        "from torch import nn\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "# other\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from urllib import request, error\r\n",
        "from time import time\r\n",
        "import random\r\n",
        "from PIL import Image\r\n",
        "from io import BytesIO\r\n",
        "from matplotlib.pyplot import imread\r\n",
        "from scipy.ndimage.interpolation import zoom, rotate\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Device configuration, as before\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImFMbbzcsil2"
      },
      "source": [
        "#Our try with ori#\r\n",
        "\r\n",
        "#This wlll be a reference to the other 2 versions of the building of the dataset\r\n",
        "\r\n",
        "the main dunction are:\r\n",
        "\r\n",
        "1) init\r\n",
        "\r\n",
        "2) getitem\r\n",
        "\r\n",
        "3) data function such as \"flip\" \"crop\" \"normalize\" and so on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUyTQ1Uk70zj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "be91e2c8-c5f0-47c2-effb-62ab35b94f61"
      },
      "source": [
        "\r\n",
        "class TrainDataset(Dataset):\r\n",
        "    def __init__(self, imgs_path, targets_path):  # will have targets here?\r\n",
        "        self.imgs_path = pd.read_csv(imgs_path)\r\n",
        "        self.target_file = pd.read_csv(targets_path)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        # return how many items in dataset\r\n",
        "        # ...\r\n",
        "        return\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        current_image = image.load(self.imgs_path + str(index) + '.png')\r\n",
        "        current_target = self.target_file[index]\r\n",
        "\r\n",
        "        ### Transforms:\r\n",
        "        # resize\r\n",
        "        # normalization (x - mean) / std\r\n",
        "        # flip randomly  -- augmentation\r\n",
        "        # random crop -- augmentation\r\n",
        "        # to tensor: current_image = torch.tensor(current_image)  (torch.FloatTensor(current_image))\r\n",
        "\r\n",
        "        return current_image, current_target\r\n",
        "\r\n",
        "\r\n",
        "NUM_EPOCHS = 100\r\n",
        "imgs_path = \"/Ori/images/\"   #  1000 images 0.png - 999.png\r\n",
        "targets_path = \"/Ori/targets.csv\"   #  1000 images 0.png - 999.png\r\n",
        "\r\n",
        "train_dataset = TrainDataset(imgs_path, targets_path)\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\r\n",
        "\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "    # for i, (batch, target) in enumerate(train_dataloader):\r\n",
        "    for i, batch in enumerate(train_dataloader):\r\n",
        "        res = model(batch)\r\n",
        "        loss = ((res - batch) ** 2).mean()  ## MSE\r\n",
        "\r\n",
        "        if i % 10 == 0:\r\n",
        "            evaluate_model()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-91e76081a5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtargets_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Ori/targets.csv\"\u001b[0m   \u001b[0;31m#  1000 images 0.png - 999.png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-91e76081a5d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, imgs_path, targets_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrainDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# will have targets here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Ori/images/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-O_XPHawB1e"
      },
      "source": [
        "#GLRDataset\r\n",
        "\r\n",
        "this dataset is based on the git of the third and first place of the retrivel competition of 2019,\r\n",
        "\r\n",
        "so it's a pretty conplex dataset, so we are not going to use it, but we want to use and explore more of those functions:\r\n",
        "\r\n",
        "**def__init__** = initialze the datset and define the function that we need\r\n",
        "\r\n",
        "**_getitem_** = load one picture from the database and can use one of the data functions to activate on the returned data\r\n",
        "\r\n",
        "**augment** = increase the size of the pis to a \"spesific\" size\r\n",
        "\r\n",
        "**normalize_img** = normalize the pictures sizes or normalize the picture pixels  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KG4jMAJ_JR2"
      },
      "source": [
        "###------Our GLR dataset - first try 13/01------###\r\n",
        "\r\n",
        "class GLRDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, df, suffix='.jpg', preload=False, aug = None, normalization='simple'):\r\n",
        "\r\n",
        "        self.df = df  \r\n",
        "        self.aug = aug\r\n",
        "        self.normalization = normalization\r\n",
        "        self.labels = self.df.target.values\r\n",
        "        self.img_folder = self.df.img_folder.values\r\n",
        "        self.suffix = suffix\r\n",
        "        self.image_names = self.df.id.values\r\n",
        "        self.images_cache = {}\r\n",
        "        self.images_in_cache = False\r\n",
        "\r\n",
        "        if preload:\r\n",
        "            self.preload()\r\n",
        "            self.images_in_cache = True\r\n",
        "        self.eps = 1e-6\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        id_ = self.image_names[idx]\r\n",
        "        img_folder_ = self.img_folder[idx]\r\n",
        "        \r\n",
        "        if self.images_in_cache:\r\n",
        "            img = self.images_cache[id_]\r\n",
        "        else:\r\n",
        "            img = self.load_one(id_, img_folder_)\r\n",
        "            \r\n",
        "        if self.aug:\r\n",
        "            img = self.augment(img)\r\n",
        "                \r\n",
        "        img = img.astype(np.float32)       \r\n",
        "        if self.normalization:\r\n",
        "            img = self.normalize_img(img)\r\n",
        "    \r\n",
        "        tensor = self.to_torch_tensor(img)\r\n",
        "        \r\n",
        "        target = torch.tensor(self.labels[idx])\r\n",
        "        feature_dict = {'idx':torch.tensor(idx).long(),\r\n",
        "                        'input':tensor,\r\n",
        "                       'target':target.float()}\r\n",
        "        return feature_dict\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_names)\r\n",
        "\r\n",
        "\r\n",
        "    def preload(self):\r\n",
        "        if self.n_threads > 1:\r\n",
        "            with mp.Pool(self.n_threads) as p:\r\n",
        "                imgs = p.map(self.load_one,self.id)\r\n",
        "            self.images_cache = dict(zip(self.id, imgs))\r\n",
        "        else:\r\n",
        "            for i in tqdm(self.id):\r\n",
        "                self.images_cache[i] = self.load_one(i)\r\n",
        "\r\n",
        "    def load_one(self, id_, img_folder_):\r\n",
        "        try:\r\n",
        "            img = cv2.imread(img_folder_ + f'{id_[0]}/{id_[1]}/{id_[2]}/{id_}{self.suffix}')\r\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB )\r\n",
        "        except:\r\n",
        "            print(\"FAIL READING IMG\", img_folder_ + f'{id_[0]}/{id_[1]}/{id_[2]}/{id_}{self.suffix}')\r\n",
        "            img = np.zeros((512,512,3), dtype=np.int8)\r\n",
        "        return img\r\n",
        "\r\n",
        "    def augment(self,img):\r\n",
        "        img_aug = self.aug(image=img)['image']\r\n",
        "        return img_aug.astype(np.float32)\r\n",
        "\r\n",
        "    def normalize_img(self,img):\r\n",
        "        \r\n",
        "        if self.normalization == 'channel':\r\n",
        "            pixel_mean = img.mean((0,1))\r\n",
        "            pixel_std = img.std((0,1)) + self.eps\r\n",
        "            img = (img - pixel_mean[None,None,:]) / pixel_std[None,None,:]\r\n",
        "            img = img.clip(-20,20)\r\n",
        "\r\n",
        "        elif self.normalization == 'channel_mean':\r\n",
        "            pixel_mean = img.mean((0,1))\r\n",
        "            img = (img - pixel_mean[None,None,:])\r\n",
        "            img = img.clip(-20,20)\r\n",
        "            \r\n",
        "        elif self.normalization == 'image':\r\n",
        "            img = (img - img.mean()) / img.std() + self.eps\r\n",
        "            img = img.clip(-20,20)\r\n",
        "            \r\n",
        "        elif self.normalization == 'simple':\r\n",
        "            img = img/255\r\n",
        "            \r\n",
        "        elif self.normalization == 'inception':\r\n",
        "            \r\n",
        "            mean = np.array([0.5, 0.5 , 0.5], dtype=np.float32)\r\n",
        "            std = np.array([0.5, 0.5 , 0.5], dtype=np.float32)\r\n",
        "            img = img.astype(np.float32)\r\n",
        "            img = img/255.\r\n",
        "            img -= mean\r\n",
        "            img *= np.reciprocal(std, dtype=np.float32)\r\n",
        "            \r\n",
        "        elif self.normalization == 'imagenet':\r\n",
        "            \r\n",
        "            mean = np.array([123.675, 116.28 , 103.53 ], dtype=np.float32)\r\n",
        "            std = np.array([58.395   , 57.120, 57.375   ], dtype=np.float32)\r\n",
        "            img = img.astype(np.float32)\r\n",
        "            img -= mean\r\n",
        "            img *= np.reciprocal(std, dtype=np.float32)\r\n",
        "            \r\n",
        "        else:\r\n",
        "            pass\r\n",
        "        \r\n",
        "        return img\r\n",
        "    \r\n",
        "    \r\n",
        "    def to_torch_tensor(self,img):\r\n",
        "        return torch.from_numpy(img.transpose((2, 0, 1)))\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtQgy2PW7hrE"
      },
      "source": [
        "###---dosen't needthis functiom---###\r\n",
        "\r\n",
        "# def grab(url):\r\n",
        "#     response = request.urlopen(url)\r\n",
        "#     image_data = response.read()\r\n",
        "\r\n",
        "#     img = Image.open(BytesIO(image_data))\r\n",
        "\r\n",
        "#     img = img.convert('RGB')\r\n",
        "\r\n",
        "#     return np.array(img)\r\n",
        "\r\n",
        "\r\n",
        "# def sample(dataframe):\r\n",
        "#     img = None\r\n",
        "#     while(img is None):\r\n",
        "#         try:\r\n",
        "#             img = grab(dataframe.sample(1).iloc[0, 1])\r\n",
        "#         except:\r\n",
        "#             img = None\r\n",
        "\r\n",
        "#     return img\r\n",
        "\r\n",
        "###---dosen't needthis functiom---###\r\n",
        "def read_img(img_id):\r\n",
        "    try: # Try reading the downloaded data first\r\n",
        "        return imread('./test/' + img_id + '.jpg')\r\n",
        "    except: # Otherwise, try grabbing it off the web\r\n",
        "        try:\r\n",
        "            return grab(test_dict[img_id])\r\n",
        "        except: # OK, so the image doesn't exist, just return a None\r\n",
        "            return None\r\n",
        "\r\n",
        "\r\n",
        "def random_crop(img, size):\r\n",
        "    idx = np.random.randint(0, img.shape[0] - size[0])\r\n",
        "    idy = np.random.randint(0, img.shape[1] - size[1])\r\n",
        "\r\n",
        "    return img[idx:idx + size[0], idy:idy + size[1]]\r\n",
        "\r\n",
        "\r\n",
        "class RandomFlip(object):\r\n",
        "    \"\"\"Horizontally flip the given NumPy array randomly with a given probability.\r\n",
        "    Args:\r\n",
        "        axis (positive integer): axis to flip along\r\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, axis = 0, p = 0.5):\r\n",
        "        self.axis = axis\r\n",
        "        self.p = p\r\n",
        "\r\n",
        "    def __call__(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            x (NumPy array): array to be flipped.\r\n",
        "        Returns:\r\n",
        "            NumPy array: Randomly flipped NumPy array.\r\n",
        "        \"\"\"\r\n",
        "        if random.random() < self.p:\r\n",
        "            return np.flip(x, axis = self.axis).copy()\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class RandomRotation(object):\r\n",
        "    \"\"\"Rotate the image by angle.\r\n",
        "    Args:\r\n",
        "        degrees (min, max): Range of degrees to select from.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, degrees):\r\n",
        "        self.degrees = degrees\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "        \"\"\"\r\n",
        "            img (PIL Image): Image to be rotated.\r\n",
        "        Returns:\r\n",
        "            PIL Image: Rotated image.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        angle = random.uniform(self.degrees[0], self.degrees[1])\r\n",
        "\r\n",
        "        return rotate(img, angle, order = 1, reshape = False)\r\n",
        "\r\n",
        "\r\n",
        "class CSVDataset(Dataset): # Note: All torchvision transforms are just classes with a __call__ attribute anyway, so they can be used here\r\n",
        "    def __init__(self, dataframe, directory, transforms = None, submission = False):\r\n",
        "        self.directory = directory\r\n",
        "        self.dataframe = dataframe #didn't fully understand the data frame of the picture\r\n",
        "        self.submission = submission ##the submission of the projest \r\n",
        "        self.transforms = transforms ##the type of tansformation \r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        row = self.dataframe.iloc[index] ## the \r\n",
        "        url = row['url']\r\n",
        "        idx = row['id']\r\n",
        "        if self.submission:\r\n",
        "            category = torch.LongTensor([-1])[0]\r\n",
        "        else:\r\n",
        "            category = torch.LongTensor([row['landmark_id']])[0]\r\n",
        "\r\n",
        "        img = imread(self.directory + idx + '.jpg')\r\n",
        "        #img = zoom(img, (224.0 / img.shape[0], 224.0 / img.shape[1], 1), order = 1)\r\n",
        "        #img = Image.open(self.directory + idx + '.jpg')\r\n",
        "\r\n",
        "        img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_LINEAR)\r\n",
        "        if self.transforms is not None:\r\n",
        "            img = self.transforms(img)\r\n",
        "        img = torch.from_numpy(np.transpose(img, (2, 0, 1)))\r\n",
        "\r\n",
        "        if self.submission:\r\n",
        "            return img, idx\r\n",
        "        return img, category\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "k-nD9wyt90EI",
        "outputId": "196efe11-1317-40e0-cb4a-1cdbf3f0012a"
      },
      "source": [
        "#all_data = pd.read_csv('/home/data/LandmarkRetrieval/train_clean.csv') # This has just under 100k images\r\n",
        "#train_data, val_data = split_validation(all_data, 0.8)\r\n",
        "train_data = pd.read_csv('/home/data/LandmarkRetrieval/train_split.csv') ## Ask ori how to import the csv files\r\n",
        "val_data = pd.read_csv('/home/data/LandmarkRetrieval/val_split.csv') ## Ask ori how to import the csv files\r\n",
        "\r\n",
        "\r\n",
        "# classes = int(max(np.max(val_data['landmark_id']), np.max(train_data['landmark_id']))) + 1\r\n",
        "\r\n",
        "print('Training samples: ', len(train_data), '\\n', train_data.head())\r\n",
        "print('Validation samples: ', len(val_data), '\\n', val_data.head())\r\n",
        "\r\n",
        "\r\n",
        "transforms = Compose([RandomFlip(axis = 0), RandomFlip(axis = 1), RandomRotation(degrees = (-20, 20))])\r\n",
        "\r\n",
        "train_set = CSVDataset(train_data, '/home/data/LandmarkRetrieval/train/', transforms = transforms)\r\n",
        "val_set = CSVDataset(val_data, '/home/data/LandmarkRetrieval/train/')\r\n",
        "\r\n",
        "# 16 seems to be the maximum batchsize we can do parallel load and train with\r\n",
        "train_loader = DataLoader(train_set, batch_size = 16, sampler = sampler, num_workers = 6, pin_memory = True)\r\n",
        "val_loader = DataLoader(val_set, batch_size = 16, shuffle = True, num_workers = 6, pin_memory = True)\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-620b571ab55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#all_data = pd.read_csv('/home/data/LandmarkRetrieval/train_clean.csv') # This has just under 100k images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#train_data, val_data = split_validation(all_data, 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/data/LandmarkRetrieval/train_split.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/data/LandmarkRetrieval/val_split.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'landmark_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'landmark_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/data/LandmarkRetrieval/train_split.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRHxqI432zZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ef73fd26ee914ecdb22046ed28d35daf",
            "4b06620a8a554af190b006df76f66f2f",
            "81c902b0ff0b4c4c83128f6f517407ff",
            "41bda20f9c3a49649ad01939b49550c7",
            "f5f2d35c7af74b689d4e589505de4bf2",
            "8fad7044a1bf4ce69301929193f6230f",
            "9a309c81474a473ba594d21a12a9f1d6",
            "f8fe8866e06243b3b18b23ba390bef5b"
          ]
        },
        "outputId": "9b2cd15b-16ea-4a2b-a470-67d0bfbb836b"
      },
      "source": [
        "# convert data to torch.FloatTensor\r\n",
        "transform = transforms.ToTensor()\r\n",
        "\r\n",
        "# load the training and test datasets\r\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True,\r\n",
        "                                   download=True, transform=transform)\r\n",
        "test_dataset = datasets.CIFAR10(root='data', train=False,\r\n",
        "                                  download=True, transform=transform)\r\n",
        "\r\n",
        "# prepare data loaders\r\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef73fd26ee914ecdb22046ed28d35daf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}