{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPB3dQaMHLa8M5hevrT46EE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pardodan/project_A/blob/main/Knn%20try2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "j6TSgbRx3szA",
        "outputId": "84585090-10e2-417a-dd89-cac05af5494c"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "\r\n",
        "import keras\r\n",
        "from keras.applications import ResNet50\r\n",
        "from keras.layers import Layer\r\n",
        "from keras import regularizers\r\n",
        "from keras.engine.topology import Input\r\n",
        "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, \\\r\n",
        "    GlobalAveragePooling2D, Lambda, MaxPooling2D, Reshape\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "from generators import *\r\n",
        "from utilities import *\r\n",
        "from metrics import *\r\n",
        "\r\n",
        "\r\n",
        "# ------------------------------ form the dataset ------------------------------ #\r\n",
        "\r\n",
        "download_file(\"https://s3.amazonaws.com/google-landmark/metadata/train.csv\", \"train.csv\")\r\n",
        "train = pd.read_csv(\"train.csv\")\r\n",
        "\r\n",
        "print(train.head())\r\n",
        "print(train.shape)\r\n",
        "print(\"Number of classes {}\".format(len(train.landmark_id.unique())))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "NUM_THRESHOLD = 20\r\n",
        "\r\n",
        "counts = dict(Counter(train['landmark_id']))\r\n",
        "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD and x != 138982}\r\n",
        "NUM_CLASSES = len(landmarks_dict)\r\n",
        "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\r\n",
        "\r\n",
        "i = 0\r\n",
        "landmark_to_idx = {}\r\n",
        "idx_to_landmark = []\r\n",
        "for k in landmarks_dict:\r\n",
        "    landmark_to_idx[k] = i\r\n",
        "    idx_to_landmark.append(k)\r\n",
        "    i += 1\r\n",
        "\r\n",
        "all_ids = train['id'].tolist()\r\n",
        "all_landmarks = train['landmark_id'].tolist()\r\n",
        "valid_ids_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_ids, all_landmarks) if x[1] in landmarks_dict}\r\n",
        "valid_ids_list = [x[0] for x in zip(all_ids, all_landmarks) if x[1] in landmarks_dict]\r\n",
        "\r\n",
        "NUM_EXAMPLES = len(valid_ids_list)\r\n",
        "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))\r\n",
        "\r\n",
        "\r\n",
        "# --------------------------------------- load model --------------------------------------- #\r\n",
        "\r\n",
        "# loading the model trained in recognition problem\r\n",
        "model = load_model(\"../input/resnet50-0092/resnet50.model\", custom_objects={'accuracy_class':accuracy_class})\r\n",
        "\r\n",
        "# removing the softmax layer to keep upto the global pooling layer\r\n",
        "model = Model(inputs=[model.input], outputs=[model.layers[-2].output])\r\n",
        "\r\n",
        "# ---------------------------------------- faiss -------------------------------------------- #\r\n",
        "\r\n",
        "import faiss                   # make faiss available\r\n",
        "faiss_index = faiss.IndexFlatL2(2048)   # build the index\r\n",
        "print(faiss_index.is_trained)\r\n",
        "\r\n",
        "# ------------------------------- add the index images features to faiss -------------------------------- #\r\n",
        "\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "tm = time.time()\r\n",
        "\r\n",
        "index_ids = []\r\n",
        "\r\n",
        "tar_images = []\r\n",
        "tar_ids = []\r\n",
        "\r\n",
        "\r\n",
        "def pickfiles(dirr):\r\n",
        "    count = 0\r\n",
        "    for f in os.listdir(dirr):\r\n",
        "        if os.path.isfile(dirr + \"/\" + f):\r\n",
        "            tar_images.append(dirr + \"/\" + f)\r\n",
        "            tar_ids.append(f[:-4])\r\n",
        "            count += 1\r\n",
        "        else:\r\n",
        "            count += pickfiles(dirr + \"/\" + f)\r\n",
        "    return count\r\n",
        "\r\n",
        "\r\n",
        "for tar in range(100):\r\n",
        "    if tar < 10:\r\n",
        "        tar_id = \"00\" + str(tar)\r\n",
        "    else:\r\n",
        "        tar_id = \"0\" + str(tar)\r\n",
        "\r\n",
        "    tar_images = []\r\n",
        "    tar_ids = []\r\n",
        "\r\n",
        "    download_file(\"https://s3.amazonaws.com/google-landmark/index/images_{}.tar\".format(tar_id), \"images.tar\",\r\n",
        "                  bar=False)\r\n",
        "    tar = tarfile.open('images.tar')\r\n",
        "    tar.extractall(\"imagesfolder\")\r\n",
        "    tar.close()\r\n",
        "\r\n",
        "    os.unlink(\"images.tar\")\r\n",
        "\r\n",
        "    total = pickfiles(\"imagesfolder\")\r\n",
        "    print(tar, total, len(tar_ids))\r\n",
        "\r\n",
        "    N = total\r\n",
        "    batchsize = 1000\r\n",
        "    validM = N // batchsize + int(N % batchsize > 0)\r\n",
        "    for i in range(validM):\r\n",
        "        temp = tar_images[i * batchsize:min(N, (i + 1) * batchsize)]\r\n",
        "        batch_images = []\r\n",
        "        for t in temp:\r\n",
        "            im = cv2.imread(t)\r\n",
        "            im = cv2.resize(im, (192, 192), interpolation=cv2.INTER_AREA)\r\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\r\n",
        "            batch_images.append(im)\r\n",
        "\r\n",
        "        batch_images = np.array(batch_images)\r\n",
        "\r\n",
        "        # print(\"batch_images\", batch_images.shape)\r\n",
        "        preds = model.predict(batch_images)\r\n",
        "        faiss_index.add(np.array(preds))\r\n",
        "\r\n",
        "    # final_preds.extend(preds_list)\r\n",
        "    index_ids.extend(tar_ids)\r\n",
        "    shutil.rmtree(\"imagesfolder\")\r\n",
        "\r\n",
        "print(\"time\", time.time() - tm)\r\n",
        "print(faiss_index.ntotal)\r\n",
        "\r\n",
        "\r\n",
        "# ------------------------------------ similarity search from test images --------------------------------- #\r\n",
        "\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "tm = time.time()\r\n",
        "\r\n",
        "final_ids = []\r\n",
        "final_preds = []\r\n",
        "final_dists = []\r\n",
        "\r\n",
        "tar_images = []\r\n",
        "tar_ids = []\r\n",
        "\r\n",
        "\r\n",
        "def pickfiles(dirr):\r\n",
        "    count = 0\r\n",
        "    for f in os.listdir(dirr):\r\n",
        "        if os.path.isfile(dirr + \"/\" + f):\r\n",
        "            tar_images.append(dirr + \"/\" + f)\r\n",
        "            tar_ids.append(f[:-4])\r\n",
        "            count += 1\r\n",
        "        else:\r\n",
        "            count += pickfiles(dirr + \"/\" + f)\r\n",
        "    return count\r\n",
        "\r\n",
        "\r\n",
        "for tar in range(20):\r\n",
        "    if tar < 10:\r\n",
        "        tar_id = \"00\" + str(tar)\r\n",
        "    else:\r\n",
        "        tar_id = \"0\" + str(tar)\r\n",
        "\r\n",
        "    tar_images = []\r\n",
        "    tar_ids = []\r\n",
        "\r\n",
        "    download_file(\"https://s3.amazonaws.com/google-landmark/test/images_{}.tar\".format(tar_id), \"images.tar\", bar=False)\r\n",
        "    tar = tarfile.open('images.tar')\r\n",
        "    tar.extractall(\"imagesfolder\")\r\n",
        "    tar.close()\r\n",
        "\r\n",
        "    os.unlink(\"images.tar\")\r\n",
        "\r\n",
        "    total = pickfiles(\"imagesfolder\")\r\n",
        "    print(tar, total, len(tar_ids))\r\n",
        "\r\n",
        "    N = total\r\n",
        "    batchsize = 1000\r\n",
        "    preds_list = []\r\n",
        "    dists_list = []\r\n",
        "    validM = N // batchsize + int(N % batchsize > 0)\r\n",
        "    for i in range(validM):\r\n",
        "        temp = tar_images[i * batchsize:min(N, (i + 1) * batchsize)]\r\n",
        "        batch_images = []\r\n",
        "        for t in temp:\r\n",
        "            im = cv2.imread(t)\r\n",
        "            im = cv2.resize(im, (192, 192), interpolation=cv2.INTER_AREA)\r\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\r\n",
        "            batch_images.append(im)\r\n",
        "\r\n",
        "        batch_images = np.array(batch_images)\r\n",
        "\r\n",
        "        # print(\"batch_images\", batch_images.shape)\r\n",
        "        preds = model.predict(batch_images)\r\n",
        "        for j in range(preds.shape[0]):\r\n",
        "            _, I = faiss_index.search(preds[j:j + 1], 100)\r\n",
        "            preds_list.append(I[0])\r\n",
        "\r\n",
        "    final_preds.extend(preds_list)\r\n",
        "    final_ids.extend(tar_ids)\r\n",
        "    shutil.rmtree(\"imagesfolder\")\r\n",
        "\r\n",
        "print(\"time\", time.time() - tm)\r\n",
        "\r\n",
        "# ---------------------------------------- submission --------------------------------------- #\r\n",
        "\r\n",
        "test_out = []\r\n",
        "for i in range(len(final_ids)):\r\n",
        "    preds = list(final_preds[i])\r\n",
        "    test_out.append(\" \".join([index_ids[preds[j]] for j in range(len(preds))]))\r\n",
        "\r\n",
        "outdf = pd.DataFrame({'id':final_ids, 'images':test_out})\r\n",
        "outdf.to_csv(\"submission.csv\", index=False)\r\n",
        "\r\n",
        "# ------------------------------------------ the end ---------------------------------------- #"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7ec1a35532a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'generators'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}